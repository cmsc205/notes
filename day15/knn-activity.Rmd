---
title: "K-Nearest Neighbors"
output: html_notebook
---

This activity on K-Nearest Neighbors in R is adapted from p. 165-167 of "Introduction to Statistical Learning with Applications in R" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It is also adapted from an  implementation by Amelia McNamara and R. Jordan Crouser at Smith College.

In this activity, we will perform KNN on the [`Caravan`]() data set from the `ISLR` package. The data are available on the course webpage. Let's start by loading a few useful packages.

```{r, message=FALSE}
library(tidyverse) # for data manipulation functions
library(class)     # for knn()
```

Additionally, we need to define the `normalize()` function.

```{r}
normalize <- function(df) {
  num_cols <- df %>%
    select_if(is.numeric) %>%
    purrr::map_df(function(x) as.numeric(scale(x)))
  other_cols <- df %>%
    select_if(function(x) !is.numeric(x))
  cbind(num_cols, other_cols)
}
```

# Applying KNN to Caravan Insurance Data
Let's see how the `KNN` approach performs on the `Caravan` data set, which is
part of the `ISLR` package. This data set includes 85 predictors that measure demographic characteristics for 5,822 individuals. The response variable is `Purchase`, which indicates whether or not a given individual purchases a
caravan insurance policy. In this data set, only 6% of people purchased
caravan insurance.

**Task 1.** Determine the proportion of individuals purchased caravan insurance?


Because the `KNN` classifier predicts the class of a given test observation by
identifying the observations that are nearest to it, the scale of the variables
matters. Any variables that are on a large scale will have a much larger
effect on the distance between the observations, and hence on the `KNN`
classifier, than variables that are on a small scale. 

For instance, imagine a
data set that contains two variables, salary and age (measured in dollars
and years, respectively). As far as `KNN` is concerned, a difference of \$1,000
in salary is enormous compared to a difference of 50 years in age. Consequently,
salary will drive the `KNN` classification results, and age will have
almost no effect. 

This is contrary to our intuition that a salary difference
of \$1,000 is quite small compared to an age difference of 50 years. Furthermore,
the importance of scale to the `KNN` classifier leads to another issue:
if we measured salary in Japanese yen, or if we measured age in minutes,
then weâ€™d get quite different classification results from what we get if these
two variables are measured in dollars and years.

A good way to handle this problem is to **standardize (normalize)** the data so that all
variables are given a mean of zero and a standard deviation of one. Then
all variables will be on a comparable scale. The `normalize()` function defined above does just this. 

**Task 2.** Standardize (normalize) the quantitative variables in the data set.

**Task 3.** Verify that each variable has a standard deviation of one and
a mean of zero.

**Task 4.** Split the observations into a test set, containing the first 1,000
observations, and a training set, containing the remaining observations. (Note: we could also use randomization here, but the rows of this data set are already in a random order.)

**Task 5.** Fit a `KNN` model on the training data using $K = 1$, and evaluate its
performance on the test data.


You should have found that the KNN error rate on the 1,000 test observations is just under 12%. At first glance, this may appear to be fairly good. However, since only 6% of customers purchased insurance, we could get the error rate down to 6% by always predicting `No` regardless of the values of the predictors!

Suppose that there is some non-trivial cost to trying to sell insurance
to a given individual. For instance, perhaps a salesperson must visit each
potential customer. If the company tries to sell insurance to a random
selection of customers, then the success rate will be only 6%, which may
be far too low given the costs involved. 

Instead, the company would like to try to sell insurance only to customers who 
are likely to buy it. So the overall error rate is not of interest. Instead, 
the fraction of individuals that are correctly predicted to buy insurance is of 
interest.

**Task 6.** Determine the fraction of individuals that are correctly predicted to buy insurance is of interest. How different is this than the rate that we would expect if we were simply guessing at random?

**Task 7.** So far we have only considered a 1-NN classifier. Use cross-validation to determine the preferred value of $K$. Once you have decided which $K$ to use, assess your classifier's accuracy on the test data set. How much better than random guessing is your classifier performing? (Hint: you may want to use the functions from the `caret` package that we discussed last class.)

